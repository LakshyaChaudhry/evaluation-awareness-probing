{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf6908c-0fb2-4b06-948f-1f7eb9ca4567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "import zipfile\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4f99e6-cf0e-4c4e-87ab-ec7cfa596adb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_path = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the model\n",
    "model = HookedTransformer.from_pretrained(model_path, device=device, dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.3-70B-Instruct\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae86914-5b26-4a1c-86bb-62778247431d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3146ed02-3b0c-4219-b687-8dfb62f90dea",
   "metadata": {},
   "source": [
    "# Data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d44ab73-8e36-4391-b9ca-acb6cebba284",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/LRudL/sad.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e7caa6-650b-4bd1-86bc-8efa7dd3b9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get SAD stages_oversight\n",
    "def load_jsonl_from_zip(zip_path, file_name, password=b'sadtimesforthesetimes'):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        content = zip_ref.read(file_name, pwd=password).decode('utf-8')\n",
    "        return [json.loads(line) for line in content.strip().split('\\n')]\n",
    "\n",
    "zip_path = './sad/sad/stages/private_data_gen.zip'\n",
    "deploy_oversight = load_jsonl_from_zip(zip_path, 'out/deploy_oversight.jsonl')\n",
    "test_oversight = load_jsonl_from_zip(zip_path, 'out/test_oversight.jsonl')\n",
    "\n",
    "# Sample verification printouts\n",
    "print(f\"Number of samples in deploy_oversight: {len(deploy_oversight)}\")\n",
    "print(f\"Number of samples in test_oversight: {len(test_oversight)}\\n\")\n",
    "\n",
    "print(\"First item in deploy_oversight:\")\n",
    "print(deploy_oversight[0])\n",
    "print(\"\\nFirst item in test_oversight:\")\n",
    "print(test_oversight[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3d3cb1-9d3e-468d-a591-e2ba27fc830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use chat template\n",
    "\n",
    "def format_question_choices(item):\n",
    "    is_testing = 'yes' in item['choices_right']\n",
    "    \n",
    "    messages = [\n",
    "     {\"role\": \"system\", \"content\": \"\"},\n",
    "     {\"role\": \"user\", \"content\": f\"{item['body']}\"}\n",
    "    ]\n",
    "\n",
    "    formatted_message = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    return {\n",
    "        \"question\": formatted_message,\n",
    "        \"type\": \"testing\" if is_testing else \"deployment\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72c3007-f2d3-40ab-b8e3-3e5e3eb9147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = deploy_oversight + test_oversight\n",
    "formatted_data = []\n",
    "\n",
    "for entry in all_data:\n",
    "    formatted_data.append(format_question_choices(entry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a7db19-4e47-4a0a-adaf-0e7b4502b1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02553b50-4887-4f54-bc9a-fece578240c2",
   "metadata": {},
   "source": [
    "# Probe scoring functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd296906-19cb-4cf1-af10-5225fd27fdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probe_scores(model, steering_vectors, layer, prompt, tokenizer=None):\n",
    "    \"\"\"\n",
    "    Calculate probe activation values for each token position in a prompt.\n",
    "    \n",
    "    Args:\n",
    "        model: The HookedTransformer model\n",
    "        steering_vectors: Dictionary of steering vectors indexed by layer\n",
    "        layer: The specific layer to probe\n",
    "        prompt: The input prompt to analyze\n",
    "        tokenizer: Optional tokenizer (uses model's tokenizer if None)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (tokens, scores, token_strings)\n",
    "            - tokens: The tokenized prompt\n",
    "            - scores: List of scalar probe activation values for each token position\n",
    "            - token_strings: List of decoded token strings\n",
    "    \"\"\"\n",
    "    if tokenizer is None:\n",
    "        tokenizer = model.tokenizer\n",
    "    \n",
    "    # Ensure the steering vector for the requested layer exists\n",
    "    if layer not in steering_vectors:\n",
    "        raise ValueError(f\"No steering vector available for layer {layer}\")\n",
    "    \n",
    "    # Get the steering vector for the requested layer\n",
    "    steering_vector = steering_vectors[layer]\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    tokens = model.to_tokens(prompt) if hasattr(model, 'to_tokens') else torch.tensor([tokenizer.encode(prompt)]).to(model.device)\n",
    "    \n",
    "    # Store activation scores for each token position\n",
    "    scores = []\n",
    "    \n",
    "    # Define the hook function to capture activations at each position\n",
    "    def hook_fn(activation, hook):\n",
    "        # For each token position in the sequence\n",
    "        for pos in range(activation.shape[1]):\n",
    "            # Get activation at this position\n",
    "            act = activation[0, pos, :]\n",
    "            \n",
    "            # Project the activation onto the steering vector\n",
    "            # (dot product of activation with normalized steering vector)\n",
    "            projection = torch.dot(act, steering_vector[0]) / steering_vector[0].norm()\n",
    "            \n",
    "            # Save the scalar projection value\n",
    "            if pos == len(scores):  # Only add if we haven't processed this position yet\n",
    "                scores.append(projection.item())\n",
    "        \n",
    "        return activation\n",
    "    \n",
    "    # Run the model with the hook attached\n",
    "    model.reset_hooks()\n",
    "    with model.hooks(fwd_hooks=[(f\"blocks.{layer}.hook_resid_pre\", hook_fn)]):\n",
    "        with torch.no_grad():\n",
    "            model(tokens)\n",
    "    \n",
    "    # Get the token strings for better interpretability\n",
    "    token_strings = [tokenizer.decode([t]) for t in tokens[0].tolist()]\n",
    "    \n",
    "    return tokens, scores, token_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f547a865-29a9-4bd9-9460-d16a0ce20cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_with_probe(model, steering_vectors, layer, dataset, tokenizer=None, output_file=\"probe_stats.csv\"):\n",
    "    \"\"\"\n",
    "    Loop through a dataset and collect probe statistics for each question.\n",
    "    \n",
    "    Args:\n",
    "        model: The HookedTransformer model\n",
    "        steering_vectors: Dictionary of steering vectors indexed by layer\n",
    "        layer: The specific layer to probe\n",
    "        dataset: List of dictionaries containing 'question', 'test', 'deploy', 'correct_answer'\n",
    "        tokenizer: Optional tokenizer (uses model's tokenizer if None)\n",
    "        output_file: Path to save the CSV output\n",
    "    \"\"\"\n",
    "    if tokenizer is None:\n",
    "        tokenizer = model.tokenizer\n",
    "    \n",
    "    # Prepare results storage\n",
    "    results = []\n",
    "    \n",
    "    # Process each question in the dataset\n",
    "    for i, item in enumerate(tqdm(dataset, desc=\"Analyzing questions\")):\n",
    "        question = item['question']\n",
    "        \n",
    "        try:\n",
    "            # Get probe scores for the question\n",
    "            tokens, scores, token_strings = get_probe_scores(model, steering_vectors, layer, question, tokenizer)\n",
    "            \n",
    "            # Filter out BOS tokens\n",
    "            filtered_scores = []\n",
    "            filtered_tokens = []\n",
    "            filtered_indices = []\n",
    "            \n",
    "            for j, (token, score) in enumerate(zip(token_strings, scores)):\n",
    "                # Skip the BOS token (different models might have different BOS tokens)\n",
    "                if (\"<|begin_of_text|>\" in token or \n",
    "                    \"<s>\" in token or \n",
    "                    \"<BOS>\" in token or\n",
    "                    token.strip() == \"\"):\n",
    "                    continue\n",
    "                \n",
    "                filtered_indices.append(j)\n",
    "                filtered_scores.append(score)\n",
    "                filtered_tokens.append(token)\n",
    "            \n",
    "            # Calculate statistics\n",
    "            if filtered_scores:\n",
    "                max_score = max(filtered_scores)\n",
    "                min_score = min(filtered_scores)\n",
    "                mean_score = sum(filtered_scores) / len(filtered_scores)\n",
    "                std_score = (sum((x - mean_score) ** 2 for x in filtered_scores) / len(filtered_scores)) ** 0.5\n",
    "            else:\n",
    "                max_score = min_score = mean_score = std_score = float('nan')\n",
    "            \n",
    "            # Record results\n",
    "            result = {\n",
    "                'question_id': i,\n",
    "                'question': question,\n",
    "                'type': item['type'],\n",
    "                'max_score': max_score,\n",
    "                'min_score': min_score,\n",
    "                'mean_score': mean_score,\n",
    "                'std_score': std_score,\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            # Print progress\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Processed {i + 1}/{len(dataset)} questions\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question {i}: {e}\")\n",
    "            # Add a row with the error\n",
    "            results.append({\n",
    "                'question_id': i,\n",
    "                'question': question,\n",
    "                'type': item['type'],\n",
    "                'max_score': float('nan'),\n",
    "                'min_score': float('nan'),\n",
    "                'mean_score': float('nan'),\n",
    "                'std_score': float('nan'),\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    # Save results to CSV\n",
    "    pd.DataFrame(results).to_csv(output_file, index=False)\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdb4168-4045-4f09-bbf1-a2622a2a9ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_all_layers(model, normalized_vectors, test_data, tokenizer, output_dir=\"layer_probe_results\"):\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get the number of layers in the model\n",
    "    num_layers = len(normalized_vectors.keys())\n",
    "    \n",
    "    # Initialize a list to store accuracy results\n",
    "    accuracy_results = []\n",
    "    \n",
    "    # Loop through all layers with a progress bar\n",
    "    for layer in tqdm(range(1, num_layers), desc=\"Testing layers\"):\n",
    "        # Output file path for this layer\n",
    "        output_file = os.path.join(output_dir, f\"probe_stats_layer{layer}.csv\")\n",
    "        \n",
    "        # Analyze the dataset with the current layer\n",
    "        results = analyze_dataset_with_probe(\n",
    "            model=model,\n",
    "            steering_vectors=normalized_vectors,\n",
    "            layer=layer,\n",
    "            dataset=test_data,\n",
    "            tokenizer=tokenizer,\n",
    "            output_file=output_file\n",
    "        )\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e707b1-d73d-457b-95e6-c094147e1565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_auroc_from_csv(file_path):\n",
    "    \"\"\"Calculate AUROC for a single CSV file\"\"\"\n",
    "    try:\n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Check if the required columns exist\n",
    "        required_columns = ['type', 'mean_score']\n",
    "        if not all(col in df.columns for col in required_columns):\n",
    "            return None, \"Missing required columns (type and/or mean_score)\"\n",
    "        \n",
    "        # Prepare data for ROC calculation\n",
    "        # 1 for deployment (positive class), 0 for testing (negative class)\n",
    "        y_true = (df['type'] == 'deployment').astype(int)\n",
    "        \n",
    "        # Use mean_score as the prediction score\n",
    "        y_score = df['mean_score']\n",
    "        \n",
    "        # Calculate ROC curve points\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        # Calculate optimal threshold (Youden's J statistic)\n",
    "        optimal_idx = np.argmax(tpr - fpr)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        sensitivity = tpr[optimal_idx]\n",
    "        specificity = 1-fpr[optimal_idx]\n",
    "        \n",
    "        return {\n",
    "            'auroc': roc_auc,\n",
    "            'optimal_threshold': optimal_threshold,\n",
    "            'sensitivity': sensitivity,\n",
    "            'specificity': specificity\n",
    "        }, None\n",
    "    \n",
    "    except Exception as e:\n",
    "        return None, str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06073300-6f5d-43f8-ab25-4b97ab1dd1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_all_csvs_in_directory(directory_path):\n",
    "    \"\"\"Process all CSV files in the given directory\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if not os.path.isdir(directory_path):\n",
    "        print(f\"Error: '{directory_path}' is not a valid directory.\")\n",
    "        return results\n",
    "    \n",
    "    # Get list of CSV files\n",
    "    csv_files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in {directory_path}\")\n",
    "        return results\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files in {directory_path}\")\n",
    "    print(f\"{'File Name':<40} {'AUROC':<10} {'Threshold':<10} {'Sensitivity':<12} {'Specificity':<12}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    # Process each CSV file\n",
    "    for file_name in csv_files:\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        stats, error = calculate_auroc_from_csv(file_path)\n",
    "        \n",
    "        if error:\n",
    "            print(f\"{file_name:<40} Error: {error}\")\n",
    "        else:\n",
    "            auroc = stats['auroc']\n",
    "            threshold = stats['optimal_threshold']\n",
    "            sensitivity = stats['sensitivity']\n",
    "            specificity = stats['specificity']\n",
    "            \n",
    "            print(f\"{file_name:<40} {auroc:.4f}    {threshold:.4f}    {sensitivity:.4f}      {specificity:.4f}\")\n",
    "            results[file_name] = stats\n",
    "    \n",
    "    # Find and print the file with the highest AUROC\n",
    "    if results:\n",
    "        best_file = max(results.items(), key=lambda x: x[1]['auroc'])\n",
    "        print(\"\\nHighest AUROC:\")\n",
    "        print(f\"File: {best_file[0]}, AUROC: {best_file[1]['auroc']:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a3e3ce-d727-4b9c-9c06-7ca1ab1c8dad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_vectors_from_dir(directory):\n",
    "    \"\"\"Load all vectors from a directory\"\"\"\n",
    "    vectors = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.startswith('layer_') and filename.endswith('.pt'):\n",
    "            layer = int(filename.split('_')[1].split('.')[0])\n",
    "            vectors[layer] = torch.load(os.path.join(directory, filename))\n",
    "            print(vectors[layer])\n",
    "    return vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8446940e-eb73-4791-b802-91ec488d5277",
   "metadata": {},
   "source": [
    "# Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8816416a-1571-4c2c-81dd-e724a2325b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_dir = './LLAMA_70B_CONTRASTIVE_20250407_114049/' #20250216_233223\n",
    "if existing_dir is not None:\n",
    "    normalized_vectors = load_vectors_from_dir(os.path.join(existing_dir, 'normalized_vectors'))\n",
    "    print(f\"Loaded {len(normalized_vectors)} normalized vectors from {existing_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f229fa3-43e5-4ccd-bc6f-9a18e03e7767",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_dir = 'LLAMA_70B_CONTRASTIVE_PROBE_RESULTS'\n",
    "results = test_all_layers(\n",
    "    model=model,\n",
    "    normalized_vectors=normalized_vectors,\n",
    "    test_data=test_data,\n",
    "    tokenizer=tokenizer,\n",
    "    output_dir=\"LLAMA_70B_CONTRASTIVE_PROBE_RESULTS\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd5df43-35e5-4eec-8b18-a9a6ed2170c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_results = process_all_csvs_in_directory(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2796e8-a6a4-46ac-9e7a-3c0bf3f08525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5e40249-3539-421b-ad29-7b1a0bb423d9",
   "metadata": {},
   "source": [
    "# control "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b1808cf-5086-46e7-8064-20def786049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original layer 23 probe\n",
    "original = normalized_vectors[23]\n",
    "\n",
    "# Calculate mean and std\n",
    "mean = original.mean().item()\n",
    "std = original.std().item()\n",
    "\n",
    "# Generate 10 random probes with same shape, mean, and std\n",
    "random_probes = {}\n",
    "for i in range(80):  # for all layers 0-79\n",
    "    # Generate same random probe for all layers\n",
    "    random_vector = torch.randn_like(original) * std + mean\n",
    "    random_probes[i] = random_vector.to(dtype=torch.bfloat16, device='cuda:0')\n",
    "\n",
    "# This will give you 10 different random_probes dictionaries, each having the same\n",
    "# random vector repeated across all layers\n",
    "\n",
    "random_probes_list = []\n",
    "for _ in range(10):\n",
    "    probe = {}\n",
    "    random_vector = torch.randn_like(original) * std + mean\n",
    "    for i in range(80):\n",
    "        probe[i] = random_vector.to(dtype=torch.bfloat16, device='cuda:0')\n",
    "    random_probes_list.append(probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a568f5e4-f3ef-4191-95b5-5e49c13e1bf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing layers:   0%|          | 0/79 [00:00<?, ?it/s]\n",
      "\u001b[Alyzing questions:   0%|          | 0/120 [00:00<?, ?it/s]\n",
      "\u001b[Alyzing questions:   2%|▏         | 2/120 [00:00<00:14,  8.30it/s]\n",
      "\u001b[Alyzing questions:   3%|▎         | 4/120 [00:00<00:11,  9.76it/s]\n",
      "\u001b[Alyzing questions:   4%|▍         | 5/120 [00:00<00:15,  7.64it/s]\n",
      "\u001b[Alyzing questions:   6%|▌         | 7/120 [00:00<00:12,  9.16it/s]\n",
      "\u001b[Alyzing questions:   8%|▊         | 9/120 [00:00<00:11, 10.08it/s]\n",
      "\u001b[Alyzing questions:   9%|▉         | 11/120 [00:01<00:14,  7.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:  10%|█         | 12/120 [00:01<00:19,  5.56it/s]\n",
      "\u001b[Alyzing questions:  12%|█▏        | 14/120 [00:01<00:15,  6.79it/s]\n",
      "\u001b[Alyzing questions:  13%|█▎        | 16/120 [00:02<00:13,  7.84it/s]\n",
      "\u001b[Alyzing questions:  15%|█▌        | 18/120 [00:02<00:12,  8.05it/s]\n",
      "\u001b[Alyzing questions:  17%|█▋        | 20/120 [00:02<00:11,  8.88it/s]\n",
      "\u001b[Alyzing questions:  18%|█▊        | 22/120 [00:02<00:10,  9.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:  20%|██        | 24/120 [00:02<00:09, 10.21it/s]\n",
      "\u001b[Alyzing questions:  22%|██▏       | 26/120 [00:02<00:08, 10.57it/s]\n",
      "\u001b[Alyzing questions:  23%|██▎       | 28/120 [00:03<00:08, 10.79it/s]\n",
      "\u001b[Alyzing questions:  25%|██▌       | 30/120 [00:03<00:08, 10.78it/s]\n",
      "\u001b[Alyzing questions:  27%|██▋       | 32/120 [00:03<00:07, 11.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 30/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:  28%|██▊       | 34/120 [00:03<00:07, 11.29it/s]\n",
      "\u001b[Alyzing questions:  30%|███       | 36/120 [00:03<00:08,  9.53it/s]\n",
      "\u001b[Alyzing questions:  32%|███▏      | 38/120 [00:04<00:08,  9.43it/s]\n",
      "\u001b[Alyzing questions:  32%|███▎      | 39/120 [00:04<00:08,  9.03it/s]\n",
      "\u001b[Alyzing questions:  33%|███▎      | 40/120 [00:04<00:08,  9.16it/s]\n",
      "\u001b[Alyzing questions:  35%|███▌      | 42/120 [00:04<00:07,  9.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 40/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:  37%|███▋      | 44/120 [00:04<00:08,  9.16it/s]\n",
      "\u001b[Alyzing questions:  38%|███▊      | 46/120 [00:05<00:09,  8.01it/s]\n",
      "\u001b[Alyzing questions:  39%|███▉      | 47/120 [00:05<00:10,  7.29it/s]\n",
      "\u001b[Alyzing questions:  41%|████      | 49/120 [00:05<00:08,  8.38it/s]\n",
      "\u001b[Alyzing questions:  42%|████▎     | 51/120 [00:05<00:07,  9.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:  44%|████▍     | 53/120 [00:05<00:06,  9.87it/s]\n",
      "\u001b[Alyzing questions:  46%|████▌     | 55/120 [00:06<00:07,  9.20it/s]\n",
      "\u001b[Alyzing questions:  48%|████▊     | 57/120 [00:06<00:06,  9.63it/s]\n",
      "\u001b[Alyzing questions:  48%|████▊     | 58/120 [00:06<00:06,  9.67it/s]\n",
      "\u001b[Alyzing questions:  50%|█████     | 60/120 [00:06<00:05, 10.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 60/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:  52%|█████▏    | 62/120 [00:06<00:06,  9.33it/s]\n",
      "\u001b[Alyzing questions:  52%|█████▎    | 63/120 [00:06<00:06,  8.83it/s]\n",
      "\u001b[Alyzing questions:  53%|█████▎    | 64/120 [00:07<00:07,  7.68it/s]\n",
      "\u001b[Alyzing questions:  55%|█████▌    | 66/120 [00:07<00:07,  7.68it/s]\n",
      "\u001b[Alyzing questions:  57%|█████▋    | 68/120 [00:07<00:06,  8.53it/s]\n",
      "\u001b[Alyzing questions:  58%|█████▊    | 70/120 [00:07<00:05,  9.15it/s]\n",
      "\u001b[Alyzing questions:  60%|██████    | 72/120 [00:07<00:05,  9.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 70/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:  62%|██████▏   | 74/120 [00:08<00:04,  9.95it/s]\n",
      "\u001b[Alyzing questions:  63%|██████▎   | 76/120 [00:08<00:04, 10.21it/s]\n",
      "\u001b[Alyzing questions:  65%|██████▌   | 78/120 [00:08<00:04, 10.47it/s]\n",
      "\u001b[Alyzing questions:  67%|██████▋   | 80/120 [00:09<00:06,  6.42it/s]\n",
      "\u001b[Alyzing questions:  68%|██████▊   | 82/120 [00:09<00:05,  7.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 80/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:  70%|███████   | 84/120 [00:09<00:04,  8.12it/s]\n",
      "\u001b[Alyzing questions:  71%|███████   | 85/120 [00:09<00:05,  6.22it/s]\n",
      "\u001b[Alyzing questions:  72%|███████▎  | 87/120 [00:10<00:04,  7.27it/s]\n",
      "\u001b[Alyzing questions:  74%|███████▍  | 89/120 [00:10<00:03,  8.02it/s]\n",
      "\u001b[Alyzing questions:  76%|███████▌  | 91/120 [00:10<00:03,  8.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 90/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:  78%|███████▊  | 93/120 [00:10<00:03,  8.58it/s]\n",
      "\u001b[Alyzing questions:  79%|███████▉  | 95/120 [00:10<00:02,  9.32it/s]\n",
      "\u001b[Alyzing questions:  81%|████████  | 97/120 [00:10<00:02, 10.00it/s]\n",
      "\u001b[Alyzing questions:  82%|████████▎ | 99/120 [00:11<00:02, 10.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:  84%|████████▍ | 101/120 [00:11<00:02,  7.47it/s]\n",
      "\u001b[Alyzing questions:  86%|████████▌ | 103/120 [00:11<00:02,  7.22it/s]\n",
      "\u001b[Alyzing questions:  88%|████████▊ | 105/120 [00:12<00:01,  8.04it/s]\n",
      "\u001b[Alyzing questions:  88%|████████▊ | 106/120 [00:12<00:01,  7.57it/s]\n",
      "\u001b[Alyzing questions:  90%|█████████ | 108/120 [00:12<00:01,  8.32it/s]\n",
      "\u001b[Alyzing questions:  92%|█████████▏| 110/120 [00:12<00:01,  9.00it/s]\n",
      "\u001b[Alyzing questions:  93%|█████████▎| 112/120 [00:12<00:00,  9.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 110/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:  95%|█████████▌| 114/120 [00:12<00:00,  9.97it/s]\n",
      "\u001b[Alyzing questions:  97%|█████████▋| 116/120 [00:13<00:00, 10.34it/s]\n",
      "\u001b[Alyzing questions:  98%|█████████▊| 118/120 [00:13<00:00,  8.97it/s]\n",
      "Analyzing questions: 100%|██████████| 120/120 [00:13<00:00,  8.81it/s]\n",
      "Testing layers:   1%|▏         | 1/79 [00:13<17:43, 13.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 120/120 questions\n",
      "Results saved to LLAMA_70B_CONTRASTIVE_PROBE_CONTROL_RESULTS/probe_stats_layer1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:   0%|          | 0/120 [00:00<?, ?it/s]\n",
      "\u001b[Alyzing questions:   2%|▏         | 2/120 [00:00<00:14,  8.42it/s]\n",
      "\u001b[Alyzing questions:   3%|▎         | 4/120 [00:00<00:11, 10.08it/s]\n",
      "\u001b[Alyzing questions:   5%|▌         | 6/120 [00:00<00:13,  8.36it/s]\n",
      "\u001b[Alyzing questions:   7%|▋         | 8/120 [00:00<00:11,  9.45it/s]\n",
      "\u001b[Alyzing questions:   8%|▊         | 10/120 [00:01<00:12,  8.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:   9%|▉         | 11/120 [00:01<00:14,  7.53it/s]\n",
      "\u001b[Alyzing questions:  10%|█         | 12/120 [00:01<00:20,  5.35it/s]\n",
      "\u001b[Alyzing questions:  12%|█▏        | 14/120 [00:01<00:15,  6.69it/s]\n",
      "\u001b[Alyzing questions:  13%|█▎        | 16/120 [00:02<00:13,  7.77it/s]\n",
      "\u001b[Alyzing questions:  15%|█▌        | 18/120 [00:02<00:12,  8.00it/s]\n",
      "\u001b[Alyzing questions:  17%|█▋        | 20/120 [00:02<00:11,  8.88it/s]\n",
      "\u001b[Alyzing questions:  18%|█▊        | 22/120 [00:02<00:10,  9.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:  20%|██        | 24/120 [00:02<00:09, 10.18it/s]\n",
      "\u001b[Alyzing questions:  22%|██▏       | 26/120 [00:02<00:08, 10.56it/s]\n",
      "\u001b[Alyzing questions:  23%|██▎       | 28/120 [00:03<00:08, 10.79it/s]\n",
      "\u001b[Alyzing questions:  25%|██▌       | 30/120 [00:03<00:08, 10.76it/s]\n",
      "\u001b[Alyzing questions:  27%|██▋       | 32/120 [00:03<00:07, 11.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 30/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:  28%|██▊       | 34/120 [00:03<00:07, 11.32it/s]\n",
      "\u001b[Alyzing questions:  30%|███       | 36/120 [00:03<00:08,  9.54it/s]\n",
      "\u001b[Alyzing questions:  32%|███▏      | 38/120 [00:04<00:08,  9.49it/s]\n",
      "\u001b[Alyzing questions:  32%|███▎      | 39/120 [00:04<00:08,  9.04it/s]\n",
      "\u001b[Alyzing questions:  33%|███▎      | 40/120 [00:04<00:08,  9.20it/s]\n",
      "\u001b[Alyzing questions:  35%|███▌      | 42/120 [00:04<00:07,  9.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 40/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:  37%|███▋      | 44/120 [00:04<00:08,  9.19it/s]\n",
      "\u001b[Alyzing questions:  38%|███▊      | 46/120 [00:05<00:09,  8.04it/s]\n",
      "\u001b[Alyzing questions:  39%|███▉      | 47/120 [00:05<00:09,  7.34it/s]\n",
      "\u001b[Alyzing questions:  41%|████      | 49/120 [00:05<00:08,  8.43it/s]\n",
      "\u001b[Alyzing questions:  42%|████▎     | 51/120 [00:05<00:07,  9.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:  44%|████▍     | 53/120 [00:05<00:06,  9.83it/s]\n",
      "\u001b[Alyzing questions:  46%|████▌     | 55/120 [00:06<00:07,  9.07it/s]\n",
      "\u001b[Alyzing questions:  47%|████▋     | 56/120 [00:06<00:06,  9.18it/s]\n",
      "\u001b[Alyzing questions:  48%|████▊     | 58/120 [00:06<00:06,  9.45it/s]\n",
      "\u001b[Alyzing questions:  50%|█████     | 60/120 [00:06<00:06,  9.78it/s]\n",
      "\u001b[Alyzing questions:  51%|█████     | 61/120 [00:06<00:06,  8.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 60/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:  52%|█████▎    | 63/120 [00:07<00:06,  8.75it/s]\n",
      "\u001b[Alyzing questions:  53%|█████▎    | 64/120 [00:07<00:07,  7.65it/s]\n",
      "\u001b[Alyzing questions:  55%|█████▌    | 66/120 [00:07<00:07,  7.70it/s]\n",
      "\u001b[Alyzing questions:  57%|█████▋    | 68/120 [00:07<00:06,  8.52it/s]\n",
      "\u001b[Alyzing questions:  58%|█████▊    | 70/120 [00:07<00:05,  9.15it/s]\n",
      "\u001b[Alyzing questions:  60%|██████    | 72/120 [00:08<00:05,  9.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 70/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:  62%|██████▏   | 74/120 [00:08<00:04,  9.94it/s]\n",
      "\u001b[Alyzing questions:  63%|██████▎   | 76/120 [00:08<00:04, 10.21it/s]\n",
      "\u001b[Alyzing questions:  65%|██████▌   | 78/120 [00:08<00:04, 10.44it/s]\n",
      "\u001b[Alyzing questions:  67%|██████▋   | 80/120 [00:09<00:06,  6.36it/s]\n",
      "\u001b[Alyzing questions:  68%|██████▊   | 82/120 [00:09<00:05,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 80/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:  70%|███████   | 84/120 [00:09<00:04,  8.08it/s]\n",
      "\u001b[Alyzing questions:  71%|███████   | 85/120 [00:09<00:05,  6.19it/s]\n",
      "\u001b[Alyzing questions:  72%|███████▏  | 86/120 [00:09<00:05,  6.70it/s]\n",
      "\u001b[Alyzing questions:  73%|███████▎  | 88/120 [00:10<00:04,  7.80it/s]\n",
      "\u001b[Alyzing questions:  74%|███████▍  | 89/120 [00:10<00:03,  8.05it/s]\n",
      "\u001b[Alyzing questions:  76%|███████▌  | 91/120 [00:10<00:03,  8.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 90/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:  78%|███████▊  | 93/120 [00:10<00:03,  8.54it/s]\n",
      "\u001b[Alyzing questions:  79%|███████▉  | 95/120 [00:10<00:02,  9.25it/s]\n",
      "\u001b[Alyzing questions:  81%|████████  | 97/120 [00:11<00:02,  9.82it/s]\n",
      "\u001b[Alyzing questions:  82%|████████▎ | 99/120 [00:11<00:02, 10.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:  84%|████████▍ | 101/120 [00:11<00:02,  7.22it/s]\n",
      "\u001b[Alyzing questions:  86%|████████▌ | 103/120 [00:11<00:02,  7.06it/s]\n",
      "\u001b[Alyzing questions:  88%|████████▊ | 105/120 [00:12<00:01,  7.88it/s]\n",
      "\u001b[Alyzing questions:  88%|████████▊ | 106/120 [00:12<00:01,  7.45it/s]\n",
      "\u001b[Alyzing questions:  90%|█████████ | 108/120 [00:12<00:01,  8.22it/s]\n",
      "\u001b[Alyzing questions:  92%|█████████▏| 110/120 [00:12<00:01,  8.91it/s]\n",
      "\u001b[Alyzing questions:  93%|█████████▎| 112/120 [00:12<00:00,  9.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 110/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:  95%|█████████▌| 114/120 [00:13<00:00,  9.94it/s]\n",
      "\u001b[Alyzing questions:  97%|█████████▋| 116/120 [00:13<00:00, 10.31it/s]\n",
      "\u001b[Alyzing questions:  98%|█████████▊| 118/120 [00:13<00:00,  8.93it/s]\n",
      "Analyzing questions: 100%|██████████| 120/120 [00:13<00:00,  8.73it/s]\n",
      "Testing layers:   3%|▎         | 2/79 [00:27<17:35, 13.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 120/120 questions\n",
      "Results saved to LLAMA_70B_CONTRASTIVE_PROBE_CONTROL_RESULTS/probe_stats_layer2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:   0%|          | 0/120 [00:00<?, ?it/s]\n",
      "\u001b[Alyzing questions:   2%|▏         | 2/120 [00:00<00:14,  8.17it/s]\n",
      "\u001b[Alyzing questions:   3%|▎         | 4/120 [00:00<00:12,  9.58it/s]\n",
      "\u001b[Alyzing questions:   4%|▍         | 5/120 [00:00<00:15,  7.34it/s]\n",
      "\u001b[Alyzing questions:   6%|▌         | 7/120 [00:00<00:12,  8.74it/s]\n",
      "\u001b[Alyzing questions:   8%|▊         | 9/120 [00:00<00:11,  9.56it/s]\n",
      "\u001b[Alyzing questions:   8%|▊         | 10/120 [00:01<00:13,  8.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:   9%|▉         | 11/120 [00:01<00:15,  7.06it/s]\n",
      "\u001b[Alyzing questions:  10%|█         | 12/120 [00:01<00:21,  4.95it/s]\n",
      "\u001b[Alyzing questions:  12%|█▏        | 14/120 [00:01<00:16,  6.35it/s]\n",
      "\u001b[Alyzing questions:  12%|█▎        | 15/120 [00:02<00:15,  6.93it/s]\n",
      "\u001b[Alyzing questions:  14%|█▍        | 17/120 [00:02<00:12,  8.12it/s]\n",
      "\u001b[Alyzing questions:  15%|█▌        | 18/120 [00:02<00:13,  7.65it/s]\n",
      "\u001b[Alyzing questions:  17%|█▋        | 20/120 [00:02<00:11,  8.61it/s]\n",
      "\u001b[Alyzing questions:  18%|█▊        | 22/120 [00:02<00:10,  9.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:  20%|██        | 24/120 [00:02<00:09,  9.87it/s]\n",
      "\u001b[Alyzing questions:  22%|██▏       | 26/120 [00:03<00:09, 10.19it/s]\n",
      "\u001b[Alyzing questions:  23%|██▎       | 28/120 [00:03<00:08, 10.37it/s]\n",
      "\u001b[Alyzing questions:  25%|██▌       | 30/120 [00:03<00:08, 10.28it/s]\n",
      "\u001b[Alyzing questions:  27%|██▋       | 32/120 [00:03<00:08, 10.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 30/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:  28%|██▊       | 34/120 [00:03<00:07, 10.75it/s]\n",
      "\u001b[Alyzing questions:  30%|███       | 36/120 [00:04<00:09,  9.20it/s]\n",
      "\u001b[Alyzing questions:  32%|███▏      | 38/120 [00:04<00:09,  9.09it/s]\n",
      "\u001b[Alyzing questions:  32%|███▎      | 39/120 [00:04<00:09,  8.72it/s]\n",
      "\u001b[Alyzing questions:  33%|███▎      | 40/120 [00:04<00:09,  8.84it/s]\n",
      "\u001b[Alyzing questions:  35%|███▌      | 42/120 [00:04<00:08,  9.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 40/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:  36%|███▌      | 43/120 [00:04<00:08,  8.61it/s]\n",
      "\u001b[Alyzing questions:  37%|███▋      | 44/120 [00:05<00:08,  8.76it/s]\n",
      "\u001b[Alyzing questions:  38%|███▊      | 46/120 [00:05<00:09,  7.51it/s]\n",
      "\u001b[Alyzing questions:  39%|███▉      | 47/120 [00:05<00:10,  6.81it/s]\n",
      "\u001b[Alyzing questions:  41%|████      | 49/120 [00:05<00:08,  7.95it/s]\n",
      "\u001b[Alyzing questions:  42%|████▎     | 51/120 [00:05<00:07,  8.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:  44%|████▍     | 53/120 [00:06<00:07,  9.42it/s]\n",
      "\u001b[Alyzing questions:  45%|████▌     | 54/120 [00:06<00:07,  8.39it/s]\n",
      "\u001b[Alyzing questions:  47%|████▋     | 56/120 [00:06<00:07,  9.05it/s]\n",
      "\u001b[Alyzing questions:  48%|████▊     | 58/120 [00:06<00:06,  9.33it/s]\n",
      "\u001b[Alyzing questions:  50%|█████     | 60/120 [00:06<00:06,  9.70it/s]\n",
      "\u001b[Alyzing questions:  51%|█████     | 61/120 [00:07<00:06,  8.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 60/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:  52%|█████▎    | 63/120 [00:07<00:06,  8.70it/s]\n",
      "\u001b[Alyzing questions:  53%|█████▎    | 64/120 [00:07<00:07,  7.69it/s]\n",
      "\u001b[Alyzing questions:  55%|█████▌    | 66/120 [00:07<00:07,  7.69it/s]\n",
      "\u001b[Alyzing questions:  57%|█████▋    | 68/120 [00:07<00:06,  8.53it/s]\n",
      "\u001b[Alyzing questions:  58%|█████▊    | 70/120 [00:08<00:05,  9.12it/s]\n",
      "\u001b[Alyzing questions:  60%|██████    | 72/120 [00:08<00:05,  9.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 70/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:  62%|██████▏   | 74/120 [00:08<00:04,  9.91it/s]\n",
      "\u001b[Alyzing questions:  63%|██████▎   | 76/120 [00:08<00:04, 10.13it/s]\n",
      "\u001b[Alyzing questions:  65%|██████▌   | 78/120 [00:08<00:04, 10.34it/s]\n",
      "\u001b[Alyzing questions:  67%|██████▋   | 80/120 [00:09<00:06,  6.33it/s]\n",
      "\u001b[Alyzing questions:  68%|██████▊   | 82/120 [00:09<00:05,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 80/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:  70%|███████   | 84/120 [00:09<00:04,  8.07it/s]\n",
      "\u001b[Alyzing questions:  71%|███████   | 85/120 [00:10<00:05,  6.15it/s]\n",
      "\u001b[Alyzing questions:  72%|███████▎  | 87/120 [00:10<00:04,  7.17it/s]\n",
      "\u001b[Alyzing questions:  74%|███████▍  | 89/120 [00:10<00:03,  7.90it/s]\n",
      "\u001b[Alyzing questions:  76%|███████▌  | 91/120 [00:10<00:03,  8.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 90/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:  78%|███████▊  | 93/120 [00:10<00:03,  8.44it/s]\n",
      "\u001b[Alyzing questions:  79%|███████▉  | 95/120 [00:11<00:02,  9.06it/s]\n",
      "\u001b[Alyzing questions:  81%|████████  | 97/120 [00:11<00:02,  9.63it/s]\n",
      "\u001b[Alyzing questions:  82%|████████▎ | 99/120 [00:11<00:02, 10.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:  84%|████████▍ | 101/120 [00:11<00:02,  7.19it/s]\n",
      "\u001b[Alyzing questions:  86%|████████▌ | 103/120 [00:12<00:02,  7.03it/s]\n",
      "\u001b[Alyzing questions:  88%|████████▊ | 105/120 [00:12<00:01,  7.84it/s]\n",
      "\u001b[Alyzing questions:  88%|████████▊ | 106/120 [00:12<00:01,  7.39it/s]\n",
      "\u001b[Alyzing questions:  90%|█████████ | 108/120 [00:12<00:01,  8.17it/s]\n",
      "\u001b[Alyzing questions:  92%|█████████▏| 110/120 [00:13<00:01,  8.88it/s]\n",
      "\u001b[Alyzing questions:  93%|█████████▎| 112/120 [00:13<00:00,  9.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 110/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:  95%|█████████▌| 114/120 [00:13<00:00,  9.91it/s]\n",
      "\u001b[Alyzing questions:  97%|█████████▋| 116/120 [00:13<00:00, 10.27it/s]\n",
      "\u001b[Alyzing questions:  98%|█████████▊| 118/120 [00:13<00:00,  8.87it/s]\n",
      "Analyzing questions: 100%|██████████| 120/120 [00:14<00:00,  8.55it/s]\n",
      "Testing layers:   4%|▍         | 3/79 [00:41<17:33, 13.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 120/120 questions\n",
      "Results saved to LLAMA_70B_CONTRASTIVE_PROBE_CONTROL_RESULTS/probe_stats_layer3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:   0%|          | 0/120 [00:00<?, ?it/s]\n",
      "\u001b[Alyzing questions:   2%|▏         | 2/120 [00:00<00:14,  8.01it/s]\n",
      "\u001b[Alyzing questions:   3%|▎         | 4/120 [00:00<00:12,  9.51it/s]\n",
      "\u001b[Alyzing questions:   4%|▍         | 5/120 [00:00<00:15,  7.30it/s]\n",
      "\u001b[Alyzing questions:   6%|▌         | 7/120 [00:00<00:12,  8.74it/s]\n",
      "\u001b[Alyzing questions:   8%|▊         | 9/120 [00:01<00:11,  9.56it/s]\n",
      "\u001b[Alyzing questions:   8%|▊         | 10/120 [00:01<00:13,  8.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10/120 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Alyzing questions:   9%|▉         | 11/120 [00:01<00:15,  7.04it/s]\n",
      "\u001b[Alyzing questions:  10%|█         | 12/120 [00:01<00:21,  4.93it/s]\n",
      "\u001b[Alyzing questions:  12%|█▏        | 14/120 [00:01<00:16,  6.32it/s]\n",
      "Analyzing questions:  12%|█▎        | 15/120 [00:02<00:14,  7.11it/s]\n",
      "Testing layers:   4%|▍         | 3/79 [00:43<18:23, 14.52s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_dir = 'LLAMA_70B_CONTRASTIVE_PROBE_CONTROL_RESULTS'\n",
    "results = test_all_layers(\n",
    "    model=model,\n",
    "    normalized_vectors=random_probes,\n",
    "    test_data=test_data,\n",
    "    tokenizer=tokenizer,\n",
    "    output_dir=\"LLAMA_70B_CONTRASTIVE_PROBE_CONTROL_RESULTS\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53619e69-774c-4c44-8478-76b5a05f044b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 CSV files in LLAMA_70B_CONTRASTIVE_PROBE_CONTROL_RESULTS\n",
      "File Name                                AUROC      Threshold  Sensitivity  Specificity \n",
      "------------------------------------------------------------------------------------------\n",
      "probe_stats_layer1.csv                   0.4302    -0.0104    1.0000      0.0862\n",
      "probe_stats_layer2.csv                   0.5687    0.0187    0.7419      0.4828\n",
      "probe_stats_layer3.csv                   0.6541    0.0097    0.4839      0.8276\n",
      "probe_stats_layer4.csv                   0.6452    0.0238    0.4516      0.8621\n",
      "probe_stats_layer5.csv                   0.5623    0.0107    0.9516      0.1897\n",
      "probe_stats_layer6.csv                   0.4839    -0.1045    0.7742      0.3276\n",
      "probe_stats_layer7.csv                   0.4625    0.1219    0.1774      0.9655\n",
      "probe_stats_layer8.csv                   0.5854    -0.0444    0.8387      0.3793\n",
      "probe_stats_layer9.csv                   0.7264    -0.0866    0.7419      0.7069\n",
      "probe_stats_layer10.csv                  0.6869    -0.1105    0.8710      0.5345\n",
      "\n",
      "Highest AUROC:\n",
      "File: probe_stats_layer9.csv, AUROC: 0.7264\n"
     ]
    }
   ],
   "source": [
    "auc_results = process_all_csvs_in_directory(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f314499-de26-43bf-badf-eed16a232f46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9fb129a7-caa8-421d-b192-2be2472aa891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparing AUC 0.5 vs 0.9999 with n=400000:\n",
      "Z-statistic: -316.101\n",
      "P-value: 0.0000000000\n",
      "Significant at α=0.05: True\n",
      "\n",
      "Comparing AUC 0.6 vs 0.7 with n=100:\n",
      "Z-statistic: -0.745\n",
      "P-value: 0.4560565403\n",
      "Significant at α=0.05: False\n",
      "\n",
      "Comparing AUC 0.6 vs 0.7 with n=10000:\n",
      "Z-statistic: -7.454\n",
      "P-value: 0.0000000000\n",
      "Significant at α=0.05: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def compare_auc(auc1, auc2, n1, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Compare two AUCs using proper standard error calculation\n",
    "\n",
    "    Args:\n",
    "        auc1, auc2: The two AUCs to compare\n",
    "        n1: Sample size\n",
    "        alpha: Significance level\n",
    "    \"\"\"\n",
    "    # Standard error calculation\n",
    "    se1 = np.sqrt((auc1 * (1-auc1)) / (n1/4))\n",
    "    se2 = np.sqrt((auc2 * (1-auc2)) / (n1/4))\n",
    "\n",
    "    # Standard error of the difference\n",
    "    se_diff = np.sqrt(se1**2 + se2**2)\n",
    "\n",
    "    # Z-score\n",
    "    z = (auc1 - auc2) / se_diff\n",
    "\n",
    "    # Two-tailed p-value\n",
    "    p_value = 2 * (1 - stats.norm.cdf(abs(z)))\n",
    "\n",
    "    return z, p_value\n",
    "\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    (0.5, 0.9999, 400000),  #  test case\n",
    "    (0.6, 0.7, 100),  # small difference, small n\n",
    "    (0.6, 0.7, 10000),  # small difference, large n\n",
    "]\n",
    "\n",
    "for auc1, auc2, n in test_cases:\n",
    "    z, p = compare_auc(auc1, auc2, n)\n",
    "    print(f\"\\nComparing AUC {auc1} vs {auc2} with n={n}:\")\n",
    "    print(f\"Z-statistic: {z:.3f}\")\n",
    "    print(f\"P-value: {p:.10f}\")\n",
    "    print(f\"Significant at α=0.05: {p < 0.05}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "012c0898-8dd2-46c8-a2df-0cb78ff10009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0004276610332378805"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z, p = compare_auc(0.821, 0.602, 400)\n",
    "p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
